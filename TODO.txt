# TODO:
- report (tokenizer, common words, longest page, number of subdomains)  # 1 jericho
- respect robot.txt ## 2 mandy
- go through links in robot.txt ## 2 mandy
- it doesn't revisit pages  ### 3 history   isabel
- avoid dead URLs that return 200 status    #### 4  cedric
- take care of HTTP error 403   #### 4  cedric
Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/lib64/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/home/woom3/spacetime-crawler4py/crawler/worker.py", line 26, in run
    scraped_urls = scraper(tbd_url, resp)
  File "/home/woom3/spacetime-crawler4py/scraper.py", line 7, in scraper
    links = extract_next_links(url, resp)
  File "/home/woom3/spacetime-crawler4py/scraper.py", line 13, in extract_next_links
    html_page = urlopen(url)
  File "/usr/lib64/python3.6/urllib/request.py", line 223, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib64/python3.6/urllib/request.py", line 532, in open
    response = meth(req, response)
  File "/usr/lib64/python3.6/urllib/request.py", line 642, in http_response
    'http', request, response, code, msg, hdrs)
  File "/usr/lib64/python3.6/urllib/request.py", line 570, in error
    return self._call_chain(*args)
  File "/usr/lib64/python3.6/urllib/request.py", line 504, in _call_chain
    result = func(*args)
  File "/usr/lib64/python3.6/urllib/request.py", line 650, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden



ALL: 
- making sure that the crawler into traps
- honor politeness delay for each site
- crawl only pages with high textual info content
- avoid similar pages with no info
- avoid large files with low info value
- figure out how to store text